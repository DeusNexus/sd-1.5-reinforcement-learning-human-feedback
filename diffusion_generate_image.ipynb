{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c817690",
   "metadata": {},
   "source": [
    "# Inference with Trained LoRA\n",
    "This notebook loads a trained LoRA adapter and uses it with the Stable Diffusion pipeline to generate images based on prompts.\n",
    "\n",
    "**Key steps:**\n",
    "1. Load the base Stable Diffusion model.\n",
    "2. Load the trained LoRA adapter weights.\n",
    "3. Sample a prompt from the dataset.\n",
    "4. Generate and display the resulting image.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756aec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Imports\n",
    "import gc\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as PILImage\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import CLIPTokenizer\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b494be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup and Path Configuration ---\n",
    "# üí° NOTE: If you're running this in Colab, you need to mount your drive first!\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# project_folder = Path('/content/drive/MyDrive/My_Cool_Project')\n",
    "# dataset_root = project_folder / 'datasets/appa-real-dataset_v2'\n",
    "# lora_checkpoint_base_dir = project_folder / 'lora_training_runs'\n",
    "\n",
    "# Local paths for demonstration\n",
    "dataset_root: Path = Path('./datasets/appa-real-dataset_v2')\n",
    "lora_checkpoint_base_dir: Path = Path(\"./lora_training_runs\")\n",
    "\n",
    "# Define paths to your metadata and image data\n",
    "labels_md_train = dataset_root / 'labels_metadata_train.csv'\n",
    "ds_train = dataset_root / 'train_data'\n",
    "\n",
    "# Load the dataset metadata to generate a sample prompt\n",
    "df_md_train = pd.read_csv(labels_md_train)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# Helper function to build a prompt from a metadata row\n",
    "def build_prompt(row: pd.Series) -> str:\n",
    "    age_desc = f\"{int(row['age'])} years old\"\n",
    "    gender_desc = row['gender']\n",
    "    ethnicity_desc = row['ethnicity']\n",
    "    emotion_map = {\n",
    "        'neutral': \"with a neutral expression\",\n",
    "        'happy': \"smiling happily\",\n",
    "        'slightlyhappy': \"smiling slightly\",\n",
    "        'other': \"showing a subtle emotion\"\n",
    "    }\n",
    "    emotion_desc = emotion_map.get(row['emotion'], \"with an expression\")\n",
    "    return f\"A {age_desc} {ethnicity_desc} {gender_desc} {emotion_desc}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------\n",
    "## Core Inference Functions\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "def load_lora_for_inference(\n",
    "    lora_checkpoint_path: Path,\n",
    "    device: str = 'cuda'\n",
    ") -> Optional[StableDiffusionPipeline]:\n",
    "    \"\"\"\n",
    "    Loads the Stable Diffusion pipeline and applies the trained LoRA weights for inference.\n",
    "\n",
    "    Args:\n",
    "        lora_checkpoint_path (Path): The path to the saved LoRA adapter directory.\n",
    "        device (str): Device to load the model onto.\n",
    "\n",
    "    Returns:\n",
    "        Optional[StableDiffusionPipeline]: The configured pipeline, or None if loading fails.\n",
    "    \"\"\"\n",
    "    if not lora_checkpoint_path.exists():\n",
    "        print(f\"‚ùå Error: LoRA checkpoint path not found at {lora_checkpoint_path}.\")\n",
    "        return None\n",
    "\n",
    "    print(\"üöÄ Loading base Stable Diffusion model...\")\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # üí° IMPORTANT: Load the LoRA weights into the pipeline\n",
    "    pipe.unet.load_attn_procs(lora_checkpoint_path)\n",
    "    \n",
    "    # Move the entire pipeline to the specified device\n",
    "    pipe = pipe.to(device)\n",
    "\n",
    "    # Disable the safety checker as it can be resource-intensive and is not\n",
    "    # typically needed for fine-tuned models if you trust the dataset.\n",
    "    pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
    "\n",
    "    print(f\"‚úÖ Base model loaded and LoRA weights applied from {lora_checkpoint_path}\")\n",
    "    return pipe\n",
    "\n",
    "def generate_image_from_prompt(\n",
    "    pipe: StableDiffusionPipeline,\n",
    "    prompt: str,\n",
    "    device: str\n",
    ") -> PILImage.Image:\n",
    "    \"\"\"Generates an image from a prompt using the given pipeline.\"\"\"\n",
    "    # Ensure the model is in evaluation mode\n",
    "    pipe.unet.eval()\n",
    "\n",
    "    # Use autocasting for efficiency on GPUs and set seed for reproducibility\n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast(device_type=device):\n",
    "            result = pipe(\n",
    "                prompt,\n",
    "                num_inference_steps=75,\n",
    "                guidance_scale=7.5,\n",
    "                generator=generator\n",
    "            )\n",
    "    return result.images[0]\n",
    "\n",
    "def unload_model(pipe: StableDiffusionPipeline) -> None:\n",
    "    \"\"\"Unloads the pipeline and clears GPU memory.\"\"\"\n",
    "    print(\"üîª Unloading pipeline to free GPU memory...\")\n",
    "    del pipe\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"‚úÖ Model unloaded and GPU cache cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d69b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "##### SET RUN_ID TO GENERATE FROM ######\n",
    "######################################\n",
    "\n",
    "# You have two options:\n",
    "# 1. Load the best-performing checkpoint\n",
    "# RESUME_RUN_ID = 'af641a_run_20250805-173847'\n",
    "# lora_path = lora_checkpoint_base_dir / RESUME_RUN_ID / \"lora_checkpoints/best_lora_adapter\"\n",
    "\n",
    "# 2. Load a checkpoint from a specific epoch\n",
    "RESUME_RUN_ID = 'af641a_run_20250805-173847'\n",
    "RESUME_EPOCH = 10\n",
    "lora_path = lora_checkpoint_base_dir / RESUME_RUN_ID / f\"lora_checkpoints/lora_adapters_epoch_{RESUME_EPOCH}\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---- Load LoRA Weights and Model ----\n",
    "pipe: Optional[StableDiffusionPipeline] = load_lora_for_inference(lora_path, device)\n",
    "\n",
    "if pipe is None:\n",
    "    print(\"‚ùå Failed to load the model. Please check the paths.\")\n",
    "else:\n",
    "    # ---- Access Dataset & Sample Prompt ----\n",
    "    sample_row: pd.Series = random.choice(df_md_train.to_dict(orient=\"records\"))\n",
    "    sample_prompt: str = build_prompt(sample_row)\n",
    "\n",
    "    print(f\"\\nüîπ Generating image with prompt: {sample_prompt}\")\n",
    "\n",
    "    # ---- Generate and Display Image ----\n",
    "    img: PILImage.Image = generate_image_from_prompt(pipe, sample_prompt, device)\n",
    "\n",
    "    # ---- Display Image ----\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Generated Image\\nPrompt: {sample_prompt}\", fontsize=12, wrap=True)\n",
    "    plt.show()\n",
    "\n",
    "    # Unload model after use to free GPU memory\n",
    "    unload_model(pipe)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
