{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afaabfb9",
   "metadata": {},
   "source": [
    "# RLHF Diffusion Fine-Tuning for Age, Gender, Ethnicity, Emotion\n",
    "We will:\n",
    "1. Load dataset and metadata.\n",
    "2. Preprocess images and labels.\n",
    "3. Inject LoRA into U-Net.\n",
    "4. Use Textual Prompt Embeddings.\n",
    "5. Train the model.\n",
    "6. Inference for conditioned image generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ef49ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "# !pip install pandas Pillow tqdm torch torchvision transformers diffusers scikit-learn IProgress ipywidgets accelerate peft matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d19878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Configuration\n",
    "import os\n",
    "import gc\n",
    "import uuid\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "from PIL import Image as PILImage # Typed Annotation for Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, List, Optional, Any\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torchvision import transforms\n",
    "\n",
    "# from diffusers.models.attention_processor import LoRAAttnProcessor2_0, AttnProcessor\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8315e43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4065, 5), Valid: (1482, 5), Test: (1978, 5)\n"
     ]
    }
   ],
   "source": [
    "# RUN_ID: str = str(uuid.uuid4()).replace('-', '')[:6]\n",
    "# print(f\"RUN_ID: {RUN_ID}\")\n",
    "\n",
    "# Define Batch Size\n",
    "batch_size: int = 2\n",
    "\n",
    "# Dataset Paths\n",
    "dataset_root: Path = Path('./datasets/appa-real-dataset_v2')\n",
    "labels_md_train = dataset_root / 'labels_metadata_train.csv'\n",
    "labels_md_valid = dataset_root / 'labels_metadata_valid.csv'\n",
    "labels_md_test  = dataset_root / 'labels_metadata_test.csv'\n",
    "\n",
    "ds_train = dataset_root / 'train_data'\n",
    "ds_valid = dataset_root / 'valid_data'\n",
    "ds_test  = dataset_root / 'test_data'\n",
    "\n",
    "# Load Metadata\n",
    "df_md_train = pd.read_csv(labels_md_train)\n",
    "df_md_valid = pd.read_csv(labels_md_valid)\n",
    "df_md_test  = pd.read_csv(labels_md_test)\n",
    "\n",
    "print(f\"Train: {df_md_train.shape}, Valid: {df_md_valid.shape}, Test: {df_md_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83cc27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageWithPromptDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_md: pd.DataFrame,\n",
    "        images_dir: Path,\n",
    "        tokenizer: CLIPTokenizer,\n",
    "        transform: Optional[transforms.Compose],\n",
    "    ):\n",
    "        self.df = df_md\n",
    "        self.images_dir = images_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def build_prompt(self,row):\n",
    "        age_desc = f\"{int(row['age'])} years old\"\n",
    "        gender_desc = row['gender']\n",
    "        ethnicity_desc = row['ethnicity']\n",
    "\n",
    "        # Emotion mapping\n",
    "        emotion_map = {\n",
    "            'neutral': \"with a neutral expression\",\n",
    "            'happy': \"smiling happily\",\n",
    "            'slightlyhappy': \"smiling slightly\",\n",
    "            'other': \"showing a subtle emotion\"\n",
    "        }\n",
    "        emotion_desc = emotion_map.get(row['emotion'], \"with an expression\")  # fallback if unknown\n",
    "\n",
    "        prompt = f\"A {age_desc} {ethnicity_desc} {gender_desc} {emotion_desc}\"\n",
    "        return prompt\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = f\"{int(row['imageId']):06d}.jpg\"\n",
    "        img_path = self.images_dir / img_name\n",
    "\n",
    "        image = PILImage.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        prompt = self.build_prompt(row)\n",
    "        prompt_ids = self.tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=77).input_ids[0]\n",
    "\n",
    "        return {\n",
    "            'pixel_values': image,\n",
    "            'prompt_ids': prompt_ids\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0115ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transforms & Loaders\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),\n",
    "    transforms.Resize((512, 512)), # Stable Diffusion 1.5 resolution\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),\n",
    "    transforms.Resize((512, 512)), # Stable Diffusion 1.5 resolution\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "train_dataset = ImageWithPromptDataset(df_md_train, ds_train, tokenizer, transform=train_transform)\n",
    "valid_dataset = ImageWithPromptDataset(df_md_valid, ds_valid, tokenizer, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae340d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------\n",
    "## Helper Functions\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "def load_model(\n",
    "    lora_r: int = 64,\n",
    "    lora_alpha: int = 64,\n",
    "    lora_dropout: float = 0.05,\n",
    "    device: str = 'cuda',\n",
    "    verbose: bool = False\n",
    ") -> StableDiffusionPipeline:\n",
    "    \"\"\"\n",
    "    Loads the Stable Diffusion pipeline with LoRA configuration and ensures correct trainable parameters.\n",
    "\n",
    "    Args:\n",
    "        lora_r (int): LoRA rank.\n",
    "        lora_alpha (int): LoRA alpha scaling.\n",
    "        lora_dropout (float): Dropout probability for LoRA.\n",
    "        device (str): Device to load the model onto.\n",
    "        verbose (bool): If True, prints additional details during initialization.\n",
    "\n",
    "    Returns:\n",
    "        StableDiffusionPipeline: The configured pipeline ready for training.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Load base pipeline\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(device)\n",
    "    print(\"‚úÖ Base model loaded.\")\n",
    "\n",
    "    # 2. Define LoRA PEFT config\n",
    "    peft_lora_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=[\"to_q\", \"to_v\"],\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\"\n",
    "    )\n",
    "\n",
    "    # 3. Add LoRA Adapter\n",
    "    pipe.unet.add_adapter(adapter_name=\"age_gender_lora\", adapter_config=peft_lora_config)\n",
    "    print(\"‚úÖ LoRA adapter added.\")\n",
    "\n",
    "    # 4. Enable LoRA layers for training\n",
    "    pipe.unet.enable_lora()\n",
    "    print(\"‚úÖ LoRA adapter enabled for training.\")\n",
    "\n",
    "    # 5. Freeze VAE & Text Encoder\n",
    "    pipe.vae.requires_grad_(False)\n",
    "    pipe.text_encoder.requires_grad_(False)\n",
    "    print(\"‚úÖ VAE and Text Encoder frozen.\")\n",
    "\n",
    "    # --- üí° CRITICAL CHANGE: Check if LoRA weights are non-zero (i.e., resumed)\n",
    "    is_resumed_run = False\n",
    "    # Check if a LoRA_A weight tensor exists and its sum is non-zero\n",
    "    for name, module in pipe.unet.named_modules():\n",
    "        if hasattr(module, 'lora_A') and hasattr(module.lora_A, 'weight'):\n",
    "            if module.lora_A.weight.sum() != 0:\n",
    "                is_resumed_run = True\n",
    "                break\n",
    "    \n",
    "    # 6. Initialize LoRA_B weights if this is a NEW run and weights are zero\n",
    "    # This prevents re-initializing trained weights on a resumed run.\n",
    "    print(\"\\nüîç Initializing LoRA_B weights if this is a new run...\")\n",
    "    for name, module in pipe.unet.named_modules():\n",
    "        if hasattr(module, 'lora_B'):\n",
    "            # Check for the actual weight tensor and confirm it's a new run\n",
    "            if not is_resumed_run and hasattr(module.lora_B, 'weight') and module.lora_B.weight.abs().sum() == 0:\n",
    "                torch.nn.init.kaiming_uniform_(module.lora_B.weight)\n",
    "                if verbose:\n",
    "                    print(f\" - Initialized lora_B for {name}\")\n",
    "\n",
    "    # 7. Debug and Verification\n",
    "    print(\"\\nüîç Checking LoRA Layers Injected into UNet...\")\n",
    "    lora_param_count = 0\n",
    "    for name, param in pipe.unet.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            lora_param_count += 1\n",
    "            param_dtype = param.dtype\n",
    "            nan_inf = torch.isnan(param).any() or torch.isinf(param).any()\n",
    "            if verbose: print(f\" - {name}: shape={param.shape}, mean={param.data.mean():.6f}, std={param.data.std():.6f}, dtype={param_dtype}, requires_grad={param.requires_grad}, nan/inf={nan_inf}\")\n",
    "\n",
    "    if lora_param_count == 0:\n",
    "        raise RuntimeError(\"‚ùå No LoRA parameters found in UNet! Injection failed.\")\n",
    "\n",
    "    # 8. Ensure ONLY LoRA Layers Are Trainable\n",
    "    non_lora_trainable = [n for n, p in pipe.unet.named_parameters() if p.requires_grad and 'lora' not in n]\n",
    "    if non_lora_trainable:\n",
    "        print(\"‚ùå ERROR: Found non-LoRA parameters set to requires_grad=True:\")\n",
    "        for n in non_lora_trainable:\n",
    "            print(f\" - {n}\")\n",
    "        raise RuntimeError(\"Non-LoRA params are trainable! You must freeze them explicitly.\")\n",
    "    else:\n",
    "        print(\"‚úÖ Only LoRA layers are trainable.\")\n",
    "\n",
    "    print(\"\\nüöÄ LoRA setup successful. Ready for training loop.\")\n",
    "    return pipe\n",
    "\n",
    "def unload_model(pipe: StableDiffusionPipeline) -> None:\n",
    "    \"\"\"\n",
    "    Unloads the UNet from the pipeline and clears GPU memory.\n",
    "\n",
    "    Args:\n",
    "        pipe (StableDiffusionPipeline): The current pipeline to unload from.\n",
    "    \"\"\"\n",
    "    print(\"üîª Unloading UNet to free GPU memory...\")\n",
    "    del pipe.unet\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"‚úÖ UNet unloaded and GPU cache cleared.\")\n",
    "\n",
    "def setup_run_logging(\n",
    "    output_base_dir: Path,\n",
    "    resume_unet_checkpoint_path: Optional[Path] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Sets up the logging directory for a training run, generates a RUN_ID,\n",
    "    and determines if the run is new or resumed.\n",
    "    \"\"\"\n",
    "    is_resumed_run = False\n",
    "    run_dir = None\n",
    "    \n",
    "    if resume_unet_checkpoint_path and resume_unet_checkpoint_path.exists():\n",
    "        is_resumed_run = True\n",
    "        \n",
    "        # --- üí° Updated Logic Here ---\n",
    "        # Traverse up the path to find the run directory name.\n",
    "        # This assumes the structure is run_dir/lora_checkpoints/\n",
    "        run_dir_name = resume_unet_checkpoint_path.parent.parent.name\n",
    "        run_id = run_dir_name\n",
    "        \n",
    "        run_dir = output_base_dir / run_id\n",
    "        if not run_dir.exists():\n",
    "            print(f\"Warning: Resuming from checkpoint but expected run directory {run_dir} not found. Creating it.\")\n",
    "            run_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "        print(f\"üîÑ Resuming existing run with RUN_ID: {run_id}\")\n",
    "    else:\n",
    "        # New run\n",
    "        run_id = f\"{str(uuid.uuid4()).replace('-', '')[:6]}_run_{pd.Timestamp.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "        run_dir = output_base_dir / run_id\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"‚ú® Starting new run with RUN_ID: {run_id}\")\n",
    "    \n",
    "    history_file = run_dir / \"training_history.json\"\n",
    "    (run_dir / \"lora_checkpoints\").mkdir(exist_ok=True)\n",
    "    (run_dir / \"lora_samples\").mkdir(exist_ok=True)\n",
    "    \n",
    "    return {\n",
    "        \"run_id\": run_id,\n",
    "        \"run_dir\": run_dir,\n",
    "        \"history_file\": history_file,\n",
    "        \"is_resumed_run\": is_resumed_run\n",
    "    }\n",
    "\n",
    "def load_or_resume_history(history_file: Path, is_resumed_run: bool) -> Dict[str, List[Dict[str, Any]]]:\n",
    "    \"\"\"Loads existing training history or initializes a new one.\"\"\"\n",
    "    training_history = {\"epochs\": []}\n",
    "    if is_resumed_run and history_file.exists():\n",
    "        with open(history_file, 'r') as f:\n",
    "            training_history = json.load(f)\n",
    "        print(f\"üìä Loaded existing training history from {history_file}\")\n",
    "    return training_history\n",
    "\n",
    "def save_best_checkpoints(\n",
    "    pipe: StableDiffusionPipeline,\n",
    "    output_dir: Path,\n",
    "    current_epoch_number: int,\n",
    "    current_loss: float,\n",
    "    best_loss: float,\n",
    ") -> Tuple[bool, Path, Path]:\n",
    "    \"\"\"\n",
    "    Saves checkpoints only if the current loss is an improvement over the best loss.\n",
    "    Deletes the previous best checkpoint to save space.\n",
    "\n",
    "    Args:\n",
    "        pipe (StableDiffusionPipeline): The training pipeline.\n",
    "        output_dir (Path): The directory to save checkpoints.\n",
    "        current_epoch_number (int): The current epoch number.\n",
    "        current_loss (float): The average training loss for the current epoch.\n",
    "        best_loss (float): The best loss recorded so far.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[bool, Path, Path]: A tuple containing a boolean indicating if a save occurred,\n",
    "                                 and the paths to the saved LoRA adapter and full checkpoint.\n",
    "    \"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Do not save if the loss has not improved.\n",
    "    if current_loss >= best_loss:\n",
    "        print(f\"üìâ Epoch {current_epoch_number}: Loss did not improve. Best loss remains {best_loss:.6f}.\")\n",
    "        # Return a boolean indicating that no save happened, and dummy paths.\n",
    "        return False, Path(\"\"), Path(\"\")\n",
    "\n",
    "    # --- Loss has improved, so we save and remove the old checkpoint ---\n",
    "    print(f\"üèÜ Epoch {current_epoch_number}: Loss improved from {best_loss:.6f} to {current_loss:.6f}! Saving model...\")\n",
    "\n",
    "    # Define paths for the new best checkpoints\n",
    "    best_lora_adapter_save_path = output_dir / \"best_lora_adapter\"\n",
    "    best_full_unet_checkpoint_path = output_dir / \"best_full_unet_checkpoint.pth\"\n",
    "\n",
    "    # Remove old checkpoints if they exist\n",
    "    if best_lora_adapter_save_path.exists():\n",
    "        try:\n",
    "            shutil.rmtree(best_lora_adapter_save_path)\n",
    "            print(\"‚úÖ Removed old best LoRA adapter directory.\")\n",
    "        except OSError as e:\n",
    "            print(f\"‚ùå Error removing old best LoRA adapter: {e}\")\n",
    "\n",
    "    if best_full_unet_checkpoint_path.exists():\n",
    "        try:\n",
    "            best_full_unet_checkpoint_path.unlink()\n",
    "            print(\"‚úÖ Removed old best full U-Net checkpoint file.\")\n",
    "        except OSError as e:\n",
    "            print(f\"‚ùå Error removing old full U-Net checkpoint: {e}\")\n",
    "\n",
    "    # Save the new best checkpoints\n",
    "    pipe.unet.save_pretrained(best_lora_adapter_save_path, adapter_name=\"age_gender_lora\")\n",
    "    torch.save(pipe.unet.state_dict(), best_full_unet_checkpoint_path)\n",
    "    \n",
    "    print(f\"Saved new best LoRA adapter at {best_lora_adapter_save_path}\")\n",
    "    print(f\"Saved new best full U-Net checkpoint at {best_full_unet_checkpoint_path}\")\n",
    "\n",
    "    # Return a boolean indicating a save happened, and the new paths.\n",
    "    return True, best_lora_adapter_save_path, best_full_unet_checkpoint_path\n",
    "\n",
    "def generate_and_save_sample(\n",
    "    pipe: StableDiffusionPipeline,\n",
    "    dataset: Dataset,\n",
    "    sample_dir: Path,\n",
    "    current_epoch_number: int,\n",
    "    device: str\n",
    ") -> Tuple[PILImage.Image, str, Path, Path]:\n",
    "    \"\"\"Generates a sample image, saves it with its prompt, and returns the results.\"\"\"\n",
    "    sample_dir.mkdir(parents=True, exist_ok=True)\n",
    "    sample_row = random.choice(dataset.df.to_dict(orient=\"records\"))\n",
    "    sample_prompt: str = dataset.build_prompt(sample_row)\n",
    "    generator = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast(device):\n",
    "            image = pipe(prompt=sample_prompt, num_inference_steps=75, guidance_scale=7.5, generator=generator).images[0]\n",
    "\n",
    "    image_save_path = sample_dir / f\"sample_epoch_{current_epoch_number}.png\"\n",
    "    image.save(image_save_path)\n",
    "    print(f\"Saved sample image at {image_save_path}\")\n",
    "\n",
    "    prompt_save_path = sample_dir / f\"sample_epoch_{current_epoch_number}_prompt.txt\"\n",
    "    with open(prompt_save_path, 'w') as file:\n",
    "        file.write(sample_prompt)\n",
    "    print(f\"Saved prompt to {prompt_save_path}\")\n",
    "\n",
    "    return image, sample_prompt, image_save_path, prompt_save_path\n",
    "\n",
    "def display_image_with_prompt(image: PILImage.Image, prompt: str, current_epoch_number: int) -> None:\n",
    "    \"\"\"Displays a generated image along with its prompt using matplotlib.\"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Epoch {current_epoch_number} Sample\\nPrompt: {prompt}\", wrap=True)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "## The Refactored `train_lora_diffusion` function\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "def train_lora_diffusion(\n",
    "    num_epochs: int,\n",
    "    train_dataloader: DataLoader,\n",
    "    valid_dataloader: DataLoader,\n",
    "    dataset_for_sampling: Dataset,\n",
    "    learning_rate: float = 3e-5,\n",
    "    lora_r: int = 64,\n",
    "    lora_alpha: int = 64,\n",
    "    lora_dropout: float = 0.05,\n",
    "    gradient_checkpoint_enable: bool = True,\n",
    "    gradient_accumulation_steps: int = 1,\n",
    "    output_base_dir: Path = Path(\"./lora_training_runs\"),\n",
    "    resume_unet_checkpoint_path: Optional[Path] = None,\n",
    "    device: str = 'cuda',\n",
    "    verbose: bool = False,\n",
    "    use_gradio: bool = False\n",
    ") -> Optional[gr.Blocks]:\n",
    "\n",
    "    def _training_loop_generator():\n",
    "        # --- (Existing setup code) ---\n",
    "        run_info = setup_run_logging(output_base_dir, resume_unet_checkpoint_path)\n",
    "        run_dir, history_file, is_resumed_run = run_info[\"run_dir\"], run_info[\"history_file\"], run_info[\"is_resumed_run\"]\n",
    "        output_dir = run_dir / \"lora_checkpoints\"\n",
    "        sample_dir = run_dir / \"lora_samples\"\n",
    "\n",
    "        training_history = load_or_resume_history(history_file, is_resumed_run)\n",
    "        start_epoch = len(training_history[\"epochs\"])\n",
    "        total_epochs = start_epoch + num_epochs\n",
    "        \n",
    "        # We handle output based on the `use_gradio` flag later\n",
    "        if use_gradio:\n",
    "            yield {\"status\": f\"üöÄ Training from epoch {start_epoch + 1} for a total of {total_epochs} epochs.\"}\n",
    "        else:\n",
    "            print(f\"üöÄ Training from epoch {start_epoch + 1} for a total of {total_epochs} epochs.\")\n",
    "\n",
    "        pipe: StableDiffusionPipeline = load_model(lora_r, lora_alpha, lora_dropout, device)\n",
    "        pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
    "\n",
    "        # NOTE: All print() statements are replaced with a conditional yield/print.\n",
    "        if gradient_checkpoint_enable:\n",
    "            status_msg = \"‚úÖ Gradient checkpointing enabled.\"\n",
    "        else:\n",
    "            status_msg = \"‚úÖ Gradient checkpointing is not enabled.\"\n",
    "        if use_gradio: yield {\"status\": status_msg}\n",
    "        else: print(status_msg)\n",
    "        \n",
    "        lora_params = [p for n, p in pipe.unet.named_parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.AdamW(lora_params, lr=learning_rate)\n",
    "        scaler = torch.amp.GradScaler(device_type=device)\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=1e-6)\n",
    "        \n",
    "        if use_gradio: yield {\"status\": \"‚úÖ Using CosineAnnealingWarmRestarts scheduler.\"}\n",
    "        else: print(\"‚úÖ Using CosineAnnealingWarmRestarts scheduler.\")\n",
    "\n",
    "        # --- Initialize best_loss ---\n",
    "        best_loss = float('inf')\n",
    "        if is_resumed_run and training_history['epochs']:\n",
    "            best_loss = min(e['train_loss'] for e in training_history['epochs'])\n",
    "            if use_gradio: yield {\"status\": f\"üìä Resuming from history. Initial best loss: {best_loss:.6f}\"}\n",
    "            else: print(f\"üìä Resuming from history. Initial best loss: {best_loss:.6f}\")\n",
    "\n",
    "        # --- Main Training Loop ---\n",
    "        for epoch in range(start_epoch, total_epochs):\n",
    "            pipe.unet.train()\n",
    "            total_loss = 0.0\n",
    "            processed_batches = 0\n",
    "            current_epoch_number = epoch + 1\n",
    "\n",
    "            tqdm_pbar = tqdm(train_dataloader, desc=f\"Epoch {current_epoch_number}/{total_epochs}\")\n",
    "            for batch_idx, batch in enumerate(tqdm_pbar):\n",
    "                # ... (your existing batch training logic remains the same) ...\n",
    "                pixel_values = batch['pixel_values'].to(device, dtype=torch.float32)\n",
    "                with torch.no_grad():\n",
    "                    latents = pipe.vae.to(dtype=torch.float32).encode(pixel_values).latent_dist.sample()\n",
    "                    latents = latents.to(dtype=torch.float16) * 0.18215\n",
    "                noise = torch.randn_like(latents)\n",
    "                timesteps = torch.randint(10, pipe.scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n",
    "                noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "                prompt_ids = batch['prompt_ids'].to(device)\n",
    "                with torch.no_grad():\n",
    "                    encoder_hidden_states = pipe.text_encoder(prompt_ids)[0]\n",
    "                with torch.amp.autocast(device_type=device,dtype=torch.float16):\n",
    "                    model_pred = pipe.unet(noisy_latents.to(dtype=torch.float16), timesteps, encoder_hidden_states=encoder_hidden_states.to(dtype=torch.float16)).sample\n",
    "                loss = nn.functional.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "                scaler.scale(loss).backward()\n",
    "                total_loss += loss.item() * gradient_accumulation_steps\n",
    "                if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == len(train_dataloader):\n",
    "                    for p in lora_params:\n",
    "                        if p.grad is not None:\n",
    "                            p.grad.data = p.grad.data.to(dtype=torch.float32)\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(lora_params, max_norm=1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "                    processed_batches += gradient_accumulation_steps\n",
    "            \n",
    "            # --- End of Epoch Logic (Updated) ---\n",
    "            avg_loss = total_loss / processed_batches if processed_batches > 0 else 0.0\n",
    "            scheduler.step()\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "            # --- Conditional Checkpoint Saving ---\n",
    "            is_saved, lora_path, unet_path = save_best_checkpoints(\n",
    "                pipe, output_dir, current_epoch_number, avg_loss, best_loss\n",
    "            )\n",
    "            \n",
    "            if is_saved:\n",
    "                best_loss = avg_loss\n",
    "            \n",
    "            # The sample image is still generated every epoch for monitoring\n",
    "            image, prompt, image_path, prompt_path = generate_and_save_sample(\n",
    "                pipe, dataset_for_sampling, sample_dir, current_epoch_number, device\n",
    "            )\n",
    "\n",
    "            epoch_history = {\n",
    "                \"epoch\": current_epoch_number,\n",
    "                \"train_loss\": avg_loss,\n",
    "                \"best_loss\": best_loss,\n",
    "                \"saved_this_epoch\": is_saved,\n",
    "                \"sample_prompt\": prompt,\n",
    "                \"sample_image_path\": str(image_path),\n",
    "                \"lora_adapter_path\": str(lora_path) if is_saved else None,\n",
    "                \"full_unet_checkpoint_path\": str(unet_path) if is_saved else None,\n",
    "                \"learning_rate\": current_lr\n",
    "            }\n",
    "            training_history[\"epochs\"].append(epoch_history)\n",
    "\n",
    "            with open(history_file, 'w') as f:\n",
    "                json.dump(training_history, f, indent=4)\n",
    "            \n",
    "            if use_gradio:\n",
    "                yield {\n",
    "                    \"epoch\": current_epoch_number,\n",
    "                    \"train_loss\": avg_loss,\n",
    "                    \"learning_rate\": current_lr,\n",
    "                    \"sample_image\": image,\n",
    "                    \"sample_prompt\": prompt,\n",
    "                    \"history_log\": epoch_history,\n",
    "                    \"status\": f\"‚úÖ Epoch {current_epoch_number}/{total_epochs} completed. Loss: {avg_loss:.6f}. Best Loss: {best_loss:.6f}\"\n",
    "                }\n",
    "            else:\n",
    "                print(f\"Epoch [{current_epoch_number}/{total_epochs}] Average Loss: {avg_loss:.6f}, Learning Rate: {current_lr:.6e}\")\n",
    "                print(f\"Best Loss so far: {best_loss:.6f}\")\n",
    "                print(f\"Updated training history saved to {history_file}\")\n",
    "                display_image_with_prompt(image, prompt, current_epoch_number)\n",
    "                \n",
    "        unload_model(pipe)\n",
    "        if use_gradio:\n",
    "            yield {\"status\": \"‚ú® Training complete! Model unloaded.\"}\n",
    "        else:\n",
    "            print(\"‚ú® Training complete! Model unloaded.\")\n",
    "            \n",
    "    # --- Conditional Execution based on `use_gradio` flag (remains the same) ---\n",
    "    if use_gradio:\n",
    "        # ... (Gradio UI setup code remains the same) ...\n",
    "        with gr.Blocks(title=\"LoRA Training Dashboard\") as demo:\n",
    "            gr.Markdown(\"# LoRA Training Dashboard üöÄ\")\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    with gr.Accordion(\"Training Settings\", open=True):\n",
    "                        num_epochs_input = gr.Slider(label=\"Number of Epochs\", minimum=1, maximum=100, value=num_epochs, step=1)\n",
    "                        lr_input = gr.Number(label=\"Learning Rate\", value=learning_rate)\n",
    "                        lora_r_input = gr.Slider(label=\"LoRA Rank (r)\", minimum=1, maximum=256, value=lora_r, step=1)\n",
    "                        lora_alpha_input = gr.Slider(label=\"LoRA Alpha\", minimum=1, maximum=256, value=lora_alpha, step=1)\n",
    "                        lora_dropout_input = gr.Number(label=\"LoRA Dropout\", value=lora_dropout)\n",
    "                        grad_accum_input = gr.Slider(label=\"Gradient Accumulation Steps\", minimum=1, maximum=16, value=gradient_accumulation_steps, step=1)\n",
    "                    start_button = gr.Button(\"Start Training\", variant=\"primary\")\n",
    "                    status_box = gr.Textbox(label=\"Status\", lines=2)\n",
    "                    history_box = gr.JSON(label=\"Last Epoch Details\")\n",
    "                with gr.Column(scale=2):\n",
    "                    gr.Markdown(\"### Training Loss\")\n",
    "                    loss_plot = gr.Plot(x=\"Epoch\", y=\"Loss\", title=\"Training Loss over Epochs\", interactive=False, value=pd.DataFrame({\"Epoch\": [], \"Loss\": []}))\n",
    "                    gr.Markdown(\"### Sample Generation\")\n",
    "                    with gr.Row():\n",
    "                        sample_image_output = gr.Image(label=\"Generated Sample\", width=512)\n",
    "                        sample_prompt_output = gr.Textbox(label=\"Prompt\", lines=5)\n",
    "\n",
    "            training_history_df = {\"Epoch\": [], \"Loss\": []}\n",
    "            def update_gradio_plot_and_status(generator_outputs):\n",
    "                nonlocal training_history_df\n",
    "                if \"train_loss\" in generator_outputs:\n",
    "                    training_history_df[\"Epoch\"].append(generator_outputs[\"epoch\"])\n",
    "                    training_history_df[\"Loss\"].append(generator_outputs[\"train_loss\"])\n",
    "                    df = pd.DataFrame(training_history_df)\n",
    "                    return {\n",
    "                        loss_plot: gr.update(value=df),\n",
    "                        sample_image_output: gr.update(value=generator_outputs[\"sample_image\"]),\n",
    "                        sample_prompt_output: gr.update(value=generator_outputs[\"sample_prompt\"]),\n",
    "                        status_box: gr.update(value=generator_outputs[\"status\"]),\n",
    "                        history_box: gr.update(value=generator_outputs[\"history_log\"])\n",
    "                    }\n",
    "                else:\n",
    "                    return {\n",
    "                        status_box: gr.update(value=generator_outputs[\"status\"]),\n",
    "                    }\n",
    "\n",
    "            start_button.click(\n",
    "                fn=_training_loop_generator,\n",
    "                inputs=[\n",
    "                    num_epochs_input, lr_input, lora_r_input, lora_alpha_input, lora_dropout_input, grad_accum_input\n",
    "                ],\n",
    "                outputs=[loss_plot, sample_image_output, sample_prompt_output, status_box, history_box],\n",
    "            )\n",
    "        \n",
    "        return demo\n",
    "    else:\n",
    "        for _ in _training_loop_generator():\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "# --- Example Usage ---\n",
    "# In a Jupyter Notebook:\n",
    "# demo = train_lora_diffusion(..., use_gradio=True)\n",
    "# if demo:\n",
    "#     demo.launch()\n",
    "\n",
    "# In a standard Python script or for backward compatibility:\n",
    "# train_lora_diffusion(..., use_gradio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f95e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_lora_diffusion(\n",
    "#     num_epochs: int,\n",
    "#     train_dataloader: DataLoader,\n",
    "#     valid_dataloader: DataLoader,\n",
    "#     dataset_for_sampling: Dataset,\n",
    "#     learning_rate: float = 3e-5,\n",
    "#     lora_r: int = 64,\n",
    "#     lora_alpha: int = 64,\n",
    "#     lora_dropout: float = 0.05,\n",
    "#     gradient_checkpoint_enable: bool = True,\n",
    "#     gradient_accumulation_steps: int = 1,\n",
    "#     output_base_dir: Path = Path(\"./lora_training_runs\"),\n",
    "#     resume_unet_checkpoint_path: Optional[Path] = None,\n",
    "#     device: str = 'cuda',\n",
    "#     verbose: bool = False,\n",
    "# ) -> None:\n",
    "#     \"\"\"\n",
    "#     Refactored training loop for a Stable Diffusion model with LoRA.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # ... (Model and optimizer setup remains the same) ...\n",
    "#     run_info = setup_run_logging(output_base_dir, resume_unet_checkpoint_path)\n",
    "#     run_dir, history_file, is_resumed_run = run_info[\"run_dir\"], run_info[\"history_file\"], run_info[\"is_resumed_run\"]\n",
    "#     output_dir = run_dir / \"lora_checkpoints\"\n",
    "#     sample_dir = run_dir / \"lora_samples\"\n",
    "\n",
    "#     training_history = load_or_resume_history(history_file, is_resumed_run)\n",
    "#     start_epoch = len(training_history[\"epochs\"])\n",
    "#     total_epochs = start_epoch + num_epochs\n",
    "#     print(f\"üöÄ Training from epoch {start_epoch + 1} for a total of {total_epochs} epochs.\")\n",
    "\n",
    "#     pipe: StableDiffusionPipeline = load_model(lora_r, lora_alpha, lora_dropout, device)\n",
    "#     pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
    "\n",
    "#     if gradient_checkpoint_enable:\n",
    "#         pipe.unet.enable_gradient_checkpointing()\n",
    "#         print(f\"‚úÖ Gradient checkpointing enabled.\")\n",
    "\n",
    "#     if resume_unet_checkpoint_path and resume_unet_checkpoint_path.exists():\n",
    "#         unet_state_dict = torch.load(resume_unet_checkpoint_path, map_location='cpu')\n",
    "#         pipe.unet.load_state_dict(unet_state_dict)\n",
    "#         print(\"‚úÖ Full U-Net state loaded for resuming training.\")\n",
    "#     elif resume_unet_checkpoint_path and not resume_unet_checkpoint_path.exists():\n",
    "#         print(f\"‚ùå Resume checkpoint not found. Starting from scratch.\")\n",
    "\n",
    "#     # Cast all trainable LoRA parameters to FP32 explicitly\n",
    "#     for n, p in pipe.unet.named_parameters():\n",
    "#         if \"lora\" in n:\n",
    "#             p.requires_grad = True\n",
    "#             p.data = p.data.to(dtype=torch.float32)\n",
    "#         else:\n",
    "#             p.requires_grad = False\n",
    "            \n",
    "#     lora_params = [p for n, p in pipe.unet.named_parameters() if p.requires_grad]\n",
    "#     optimizer = torch.optim.AdamW(lora_params, lr=learning_rate)\n",
    "#     scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "#     scheduler = CosineAnnealingWarmRestarts(\n",
    "#         optimizer,\n",
    "#         T_0=5,\n",
    "#         T_mult=2,\n",
    "#         eta_min=1e-6\n",
    "#     )\n",
    "#     print(\"‚úÖ Using CosineAnnealingWarmRestarts scheduler.\")\n",
    "\n",
    "#     # --- Main Training Loop ---\n",
    "#     for epoch in range(start_epoch, total_epochs):\n",
    "#         pipe.unet.train()\n",
    "#         total_loss = 0.0\n",
    "#         processed_batches = 0\n",
    "#         current_epoch_number = epoch + 1\n",
    "\n",
    "#         for batch_idx, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {current_epoch_number}/{total_epochs}\")):\n",
    "#             pixel_values = batch['pixel_values'].to(device, dtype=torch.float32)\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 latents = pipe.vae.to(dtype=torch.float32).encode(pixel_values).latent_dist.sample()\n",
    "#                 latents = latents.to(dtype=torch.float16) * 0.18215\n",
    "            \n",
    "#             noise = torch.randn_like(latents)\n",
    "#             timesteps = torch.randint(10, pipe.scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n",
    "#             noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "#             prompt_ids = batch['prompt_ids'].to(device)\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 encoder_hidden_states = pipe.text_encoder(prompt_ids)[0]\n",
    "            \n",
    "#             # Use autocast only for the UNet forward pass to leverage FP16 for speed\n",
    "#             with torch.amp.autocast(device_type=device,dtype=torch.float16):\n",
    "#                 model_pred = pipe.unet(\n",
    "#                     noisy_latents.to(dtype=torch.float16), \n",
    "#                     timesteps, \n",
    "#                     encoder_hidden_states=encoder_hidden_states.to(dtype=torch.float16)\n",
    "#                 ).sample\n",
    "            \n",
    "#             # The loss must be computed with FP32 to ensure stable gradients\n",
    "#             loss = nn.functional.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "#             loss = loss / gradient_accumulation_steps\n",
    "            \n",
    "#             # Use the scaler to handle the backward pass and gradient scaling\n",
    "#             scaler.scale(loss).backward()\n",
    "#             total_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "#             # --- Gradient Accumulation Logic ---\n",
    "#             if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == len(train_dataloader):\n",
    "#                 # Before unscaling, cast the gradients of trainable parameters to FP32\n",
    "#                 for p in lora_params:\n",
    "#                     if p.grad is not None:\n",
    "#                         p.grad.data = p.grad.data.to(dtype=torch.float32)\n",
    "\n",
    "#                 scaler.unscale_(optimizer)\n",
    "#                 torch.nn.utils.clip_grad_norm_(lora_params, max_norm=1.0)\n",
    "#                 scaler.step(optimizer)\n",
    "#                 scaler.update()\n",
    "#                 optimizer.zero_grad()\n",
    "#                 processed_batches += gradient_accumulation_steps\n",
    "\n",
    "#         # ... (End of epoch logic remains the same) ...\n",
    "#         if processed_batches > 0:\n",
    "#             avg_loss = total_loss / processed_batches\n",
    "#         else:\n",
    "#             avg_loss = 0.0\n",
    "#             print(\"Warning: No batches were processed in this epoch due to NaNs/Infs.\")\n",
    "        \n",
    "#         scheduler.step()\n",
    "        \n",
    "#         current_lr = optimizer.param_groups[0]['lr']\n",
    "#         print(f\"Epoch [{current_epoch_number}/{total_epochs}] Average Loss: {avg_loss:.6f}, Learning Rate: {current_lr:.6e}\")\n",
    "\n",
    "#         lora_path, unet_path = save_checkpoints(pipe, output_dir, current_epoch_number)\n",
    "#         image, prompt, image_path, prompt_path = generate_and_save_sample(\n",
    "#             pipe, dataset_for_sampling, sample_dir, current_epoch_number, device\n",
    "#         )\n",
    "\n",
    "#         display_image_with_prompt(image, prompt, current_epoch_number)\n",
    "\n",
    "#         epoch_history = {\n",
    "#             \"epoch\": current_epoch_number,\n",
    "#             \"train_loss\": avg_loss,\n",
    "#             \"sample_prompt\": prompt,\n",
    "#             \"sample_image_path\": str(image_path),\n",
    "#             \"lora_adapter_path\": str(lora_path),\n",
    "#             \"full_unet_checkpoint_path\": str(unet_path)\n",
    "#         }\n",
    "#         training_history[\"epochs\"].append(epoch_history)\n",
    "\n",
    "#         with open(history_file, 'w') as f:\n",
    "#             json.dump(training_history, f, indent=4)\n",
    "#         print(f\"Updated training history saved to {history_file}\")\n",
    "\n",
    "#     unload_model(pipe)\n",
    "#     print(\"‚ú® Training complete! Model unloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01035093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------------------------------------------------------------------------------\n",
    "# ## Updated `train_lora_diffusion` function with Gradient Checkpointing\n",
    "# # --------------------------------------------------------------------------------------------------\n",
    "\n",
    "# def train_lora_diffusion(\n",
    "#     num_epochs: int,\n",
    "#     train_dataloader: DataLoader,\n",
    "#     valid_dataloader: DataLoader,\n",
    "#     dataset_for_sampling: Dataset,\n",
    "#     learning_rate: float = 3e-5,\n",
    "#     lora_r: int = 64,\n",
    "#     lora_alpha: int = 64,\n",
    "#     lora_dropout: float = 0.05,\n",
    "#     gradient_checkpoint_enable: bool = True,\n",
    "#     gradient_accumulation_steps: int = 1,\n",
    "#     output_base_dir: Path = Path(\"./lora_training_runs\"),\n",
    "#     resume_unet_checkpoint_path: Optional[Path] = None,\n",
    "#     device: str = 'cuda',\n",
    "#     verbose: bool = False,\n",
    "# ) -> None:\n",
    "#     \"\"\"\n",
    "#     Trains a Stable Diffusion model with LoRA for age, gender, ethnicity, and emotion conditioning.\n",
    "#     Can save both LoRA adapter weights and full U-Net checkpoints for resuming.\n",
    "\n",
    "#     Args:\n",
    "#         num_epochs (int): Number of training epochs to run.\n",
    "#         train_dataloader (DataLoader): DataLoader for the training dataset.\n",
    "#         valid_dataloader (DataLoader): DataLoader for the validation dataset.\n",
    "#         dataset_for_sampling (Dataset): The dataset object used for generating random prompts for sample images.\n",
    "#         learning_rate (float): Learning rate for the AdamW optimizer.\n",
    "#         lora_r (int): LoRA rank.\n",
    "#         lora_alpha (int): LoRA alpha scaling.\n",
    "#         lora_dropout (float): Dropout probability for LoRA.\n",
    "#         gradient_checkpoint_enable (bool): Whether to enable gradient checkpointing to save GPU memory.\n",
    "#         output_base_dir (Path): Base directory to save all run-specific files.\n",
    "#         resume_unet_checkpoint_path (Optional[Path]): Path to a .pth file to resume training from. If None, train from scratch.\n",
    "#         device (str): Device to perform training on ('cuda' or 'cpu').\n",
    "#         verbose (bool): If True, prints additional details during initialization.\n",
    "#         gradient_accumulation_steps (int): Number of updates steps to accumulate gradients.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # --- Setup Run Logging ---\n",
    "#     run_info = setup_run_logging(output_base_dir, resume_unet_checkpoint_path)\n",
    "#     run_id = run_info[\"run_id\"]\n",
    "#     run_dir = run_info[\"run_dir\"]\n",
    "#     history_file = run_info[\"history_file\"]\n",
    "#     is_resumed_run = run_info[\"is_resumed_run\"]\n",
    "\n",
    "#     # Adjust output_dir and sample_dir based on the specific run directory\n",
    "#     output_dir = run_dir / \"lora_checkpoints\"\n",
    "#     sample_dir = run_dir / \"lora_samples\"\n",
    "\n",
    "#     # Load existing history or start new\n",
    "#     training_history = {\"epochs\": []}\n",
    "#     if is_resumed_run and history_file.exists():\n",
    "#         with open(history_file, 'r') as f:\n",
    "#             training_history = json.load(f)\n",
    "#         print(f\"üìä Loaded existing training history from {history_file}\")\n",
    "\n",
    "#     # --- üí° CRITICAL CHANGE: Determine the starting epoch and total epochs ---\n",
    "#     start_epoch = 0\n",
    "#     if is_resumed_run and training_history[\"epochs\"]:\n",
    "#         completed_epochs = len(training_history[\"epochs\"])\n",
    "#         start_epoch = completed_epochs\n",
    "#         total_epochs = completed_epochs + num_epochs\n",
    "#         print(f\"üîÑ Resuming from epoch {start_epoch + 1}. Training for a total of {total_epochs} epochs.\")\n",
    "#     else:\n",
    "#         total_epochs = num_epochs\n",
    "#         print(f\"üöÄ Starting training from scratch for {total_epochs} epochs.\")\n",
    "\n",
    "#     pipe: StableDiffusionPipeline = load_model(lora_r=lora_r, lora_alpha=lora_alpha, lora_dropout=lora_dropout, device=device)\n",
    "#     pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
    "\n",
    "#     # --- Enable gradient checkpointing (conditional) ---\n",
    "#     if gradient_checkpoint_enable:\n",
    "#         pipe.unet.enable_gradient_checkpointing()\n",
    "#         print(f\"‚úÖ Gradient checkpointing enabled for UNet: gradient_accumulation_steps = {gradient_accumulation_steps}\")\n",
    "\n",
    "#     # Load full U-Net checkpoint if resuming training\n",
    "#     if resume_unet_checkpoint_path is not None:\n",
    "#         if resume_unet_checkpoint_path.exists():\n",
    "#             print(f\"üîÑ Resuming training from full U-Net checkpoint: {resume_unet_checkpoint_path}\")\n",
    "#             unet_state_dict = torch.load(resume_unet_checkpoint_path, map_location='cpu')\n",
    "#             pipe.unet.load_state_dict(unet_state_dict)\n",
    "#             print(\"‚úÖ Full U-Net state loaded for resuming training.\")\n",
    "#         else:\n",
    "#             print(f\"‚ùå Resume U-Net checkpoint not found at {resume_unet_checkpoint_path}. Starting training from scratch.\")\n",
    "\n",
    "#     scaler = torch.amp.GradScaler(device)\n",
    "\n",
    "#     # Initialize LoRA_B weights if zero\n",
    "#     for name, module in pipe.unet.named_modules():\n",
    "#         if hasattr(module, 'lora_B'):\n",
    "#             if isinstance(module.lora_B, torch.nn.ModuleDict):\n",
    "#                 for sub_name, sub_module in module.lora_B.items():\n",
    "#                     if hasattr(sub_module, 'weight'):\n",
    "#                         if sub_module.weight.abs().sum() == 0:\n",
    "#                             torch.nn.init.kaiming_uniform_(sub_module.weight)\n",
    "#                             if verbose: print(f\"Initialized lora_B: {name}.{sub_name}\")\n",
    "#             elif hasattr(module.lora_B, 'weight'):\n",
    "#                 if module.lora_B.weight.abs().sum() == 0:\n",
    "#                     torch.nn.init.kaiming_uniform_(module.lora_B.weight)\n",
    "#                     if verbose: print(f\"Initialized lora_B: {name}\")\n",
    "\n",
    "#     lora_params = [p for n, p in pipe.unet.named_parameters() if p.requires_grad]\n",
    "#     for p in lora_params:\n",
    "#         p.data = p.data.float()\n",
    "\n",
    "#     optimizer = torch.optim.AdamW(lora_params, lr=learning_rate)\n",
    "\n",
    "#     # --- üí° CRITICAL CHANGE: The `range` for the training loop ---\n",
    "#     print(f\"\\nüöÄ Starting training from epoch {start_epoch + 1} for a total of {total_epochs} epochs...\")\n",
    "\n",
    "#     for epoch in range(start_epoch, total_epochs):\n",
    "#         pipe.unet.train()\n",
    "#         total_loss = 0.0\n",
    "\n",
    "#         # --- üí° CRITICAL CHANGE: The `desc` for the progress bar uses total_epochs ---\n",
    "#         for batch_idx, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{total_epochs}\")):\n",
    "#             pixel_values = batch['pixel_values'].to(device, dtype=torch.float32)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 latents = pipe.vae.to(dtype=torch.float32).encode(pixel_values).latent_dist.sample()\n",
    "#                 latents = latents.to(dtype=torch.float16) * 0.18215\n",
    "\n",
    "#             if torch.isnan(latents).any() or torch.isinf(latents).any():\n",
    "#                 print(f\"[Batch {batch_idx}] Latents NaN/Inf Detected! Skipping batch.\")\n",
    "#                 continue\n",
    "\n",
    "#             noise = torch.randn_like(latents)\n",
    "#             if torch.isnan(noise).any() or torch.isinf(noise).any():\n",
    "#                 print(f\"[Batch {batch_idx}] Noise NaN/Inf Detected! Skipping batch.\")\n",
    "#                 continue\n",
    "\n",
    "#             timesteps = torch.randint(10, pipe.scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n",
    "#             noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "#             if torch.isnan(noisy_latents).any() or torch.isinf(noisy_latents).any():\n",
    "#                 print(f\"[Batch {batch_idx}] Noisy Latents NaN/Inf Detected! Skipping batch.\")\n",
    "#                 continue\n",
    "\n",
    "#             prompt_ids = batch['prompt_ids'].to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 encoder_hidden_states = pipe.text_encoder(prompt_ids)[0]\n",
    "\n",
    "#             with torch.amp.autocast(device, dtype=torch.float16):\n",
    "#                 model_pred = pipe.unet(noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states).sample\n",
    "\n",
    "#                 if torch.isnan(model_pred).any() or torch.isinf(model_pred).any():\n",
    "#                     print(f\"[Batch {batch_idx}] Model Prediction NaN/Inf Detected! Mean: {model_pred.mean()}, Std: {model_pred.std()}. Skipping batch.\")\n",
    "#                     continue\n",
    "\n",
    "#                 loss = nn.functional.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "#                 loss = loss / gradient_accumulation_steps\n",
    "\n",
    "#                 if torch.isnan(loss) or torch.isinf(loss):\n",
    "#                     print(f\"[Batch {batch_idx}] Loss NaN/Inf Detected! Skipping batch.\")\n",
    "#                     continue\n",
    "\n",
    "#             scaler.scale(loss).backward()\n",
    "#             total_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "#             if batch_idx % 100 == 0:\n",
    "#                 print(f\"  [Batch {batch_idx}] Loss: {loss.item() * gradient_accumulation_steps:.6f}\")\n",
    "\n",
    "#             if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == len(train_dataloader):\n",
    "#                 skip_step = False\n",
    "#                 for name, param in pipe.unet.named_parameters():\n",
    "#                     if param.requires_grad and param.grad is not None:\n",
    "#                         if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "#                             print(f\"[Batch {batch_idx}] NaN/Inf gradient detected in {name}. Skipping optimizer step.\")\n",
    "#                             skip_step = True\n",
    "#                             break\n",
    "\n",
    "#                 if skip_step:\n",
    "#                     optimizer.zero_grad()\n",
    "#                     continue\n",
    "\n",
    "#                 scaler.unscale_(optimizer)\n",
    "#                 torch.nn.utils.clip_grad_norm_(lora_params, max_norm=1.0)\n",
    "#                 scaler.step(optimizer)\n",
    "#                 scaler.update()\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#         # End of Epoch Logic\n",
    "#         avg_loss = total_loss / len(train_dataloader)\n",
    "#         current_epoch_number = epoch + 1\n",
    "#         print(f\"Epoch [{current_epoch_number}/{total_epochs}] Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "#         # --- üí° CRITICAL CHANGE: Use current_epoch_number for unique filenames ---\n",
    "#         epoch_lora_adapter_save_path = output_dir / f\"lora_adapters_epoch_{current_epoch_number}\"\n",
    "#         epoch_lora_adapter_save_path.mkdir(parents=True, exist_ok=True)\n",
    "#         pipe.unet.save_pretrained(epoch_lora_adapter_save_path, adapter_name=\"age_gender_lora\")\n",
    "#         print(f\"Saved LoRA adapter weights only at {epoch_lora_adapter_save_path}\")\n",
    "\n",
    "#         # --- üí° CRITICAL CHANGE: Use current_epoch_number for unique filenames ---\n",
    "#         full_unet_checkpoint_path = output_dir / f\"full_unet_checkpoint_epoch_{current_epoch_number}.pth\"\n",
    "#         torch.save(pipe.unet.state_dict(), full_unet_checkpoint_path)\n",
    "#         print(f\"Saved full U-Net checkpoint (including LoRA state) at {full_unet_checkpoint_path}\")\n",
    "\n",
    "#         # ---- Generate Sample Image ----\n",
    "#         sample_row = random.choice(dataset_for_sampling.df.to_dict(orient=\"records\"))\n",
    "#         sample_prompt: str = dataset_for_sampling.build_prompt(sample_row)\n",
    "#         generator = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             with torch.amp.autocast(device):\n",
    "#                 image = pipe(prompt=sample_prompt, num_inference_steps=75, guidance_scale=7.5, generator=generator).images[0]\n",
    "\n",
    "#         # --- üí° CRITICAL CHANGE: Use current_epoch_number for unique filenames ---\n",
    "#         image_save_path = sample_dir / f\"sample_epoch_{current_epoch_number}.png\"\n",
    "#         image.save(image_save_path)\n",
    "#         print(f\"Saved sample image at {image_save_path}\")\n",
    "\n",
    "#         # --- üí° CRITICAL CHANGE: Use current_epoch_number for unique filenames ---\n",
    "#         prompt_save_path = sample_dir / f\"sample_epoch_{current_epoch_number}_prompt.txt\"\n",
    "#         with open(prompt_save_path, 'w') as file:\n",
    "#             file.write(sample_prompt)\n",
    "#         print(f\"Saved prompt to {prompt_save_path}\")\n",
    "\n",
    "#         # --- Display Image and Prompt using Matplotlib ---\n",
    "#         plt.figure(figsize=(8, 8))\n",
    "#         plt.imshow(image)\n",
    "#         plt.title(f\"Epoch {current_epoch_number} Sample\\nPrompt: {sample_prompt}\", wrap=True)\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "#         print(f\"Prompt: {sample_prompt}\")\n",
    "\n",
    "#         # --- Save History for the current epoch ---\n",
    "#         epoch_history = {\n",
    "#             \"epoch\": current_epoch_number,\n",
    "#             \"train_loss\": avg_loss,\n",
    "#             \"sample_prompt\": sample_prompt,\n",
    "#             \"sample_image_path\": str(image_save_path),\n",
    "#             \"lora_adapter_path\": str(epoch_lora_adapter_save_path),\n",
    "#             \"full_unet_checkpoint_path\": str(full_unet_checkpoint_path)\n",
    "#         }\n",
    "#         training_history[\"epochs\"].append(epoch_history)\n",
    "\n",
    "#         with open(history_file, 'w') as f:\n",
    "#             json.dump(training_history, f, indent=4)\n",
    "#         print(f\"Updated training history saved to {history_file}\")\n",
    "\n",
    "#     unload_model(pipe)\n",
    "#     print(\"‚ú® Training complete! Model unloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c67befda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 2 years old caucasian male with a neutral expression\n"
     ]
    }
   ],
   "source": [
    "# Random Sample of ImageDataSet Prompt\n",
    "dataset = train_loader.dataset\n",
    "sample_row = random.choice(dataset.df.to_dict(orient=\"records\"))\n",
    "sample_prompt = dataset.build_prompt(sample_row)\n",
    "print(sample_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce2e97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ú® Starting new run with RUN_ID: af641a_run_20250805-173847\n",
      "üöÄ Training from epoch 1 for a total of 10 epochs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71f327807e941a18470013ec71e5957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49636/1239622916.py:283: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Base model loaded.\n",
      "‚úÖ LoRA adapter added.\n",
      "‚úÖ LoRA adapter enabled for training.\n",
      "‚úÖ VAE and Text Encoder frozen.\n",
      "\n",
      "üîç Initializing LoRA_B weights if this is a new run...\n",
      "\n",
      "üîç Checking LoRA Layers Injected into UNet...\n",
      "‚úÖ Only LoRA layers are trainable.\n",
      "\n",
      "üöÄ LoRA setup successful. Ready for training loop.\n",
      "‚úÖ Gradient checkpointing enabled.\n",
      "‚úÖ Using CosineAnnealingWarmRestarts scheduler.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1950/2033 [35:40<01:41,  1.22s/it]"
     ]
    }
   ],
   "source": [
    "# --- To train from scratch ---\n",
    "demo = train_lora_diffusion(\n",
    "    num_epochs=10,\n",
    "    train_dataloader=train_loader,\n",
    "    valid_dataloader=valid_loader,\n",
    "    dataset_for_sampling=train_dataset,\n",
    "    learning_rate=3e-4,\n",
    "    lora_r=128,\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.05,\n",
    "    gradient_checkpoint_enable=True,\n",
    "    gradient_accumulation_steps=4, # Example: accumulate gradients over 4 steps\n",
    "    output_base_dir=Path(\"./lora_training_runs\"), # Change 'output_dir' to 'output_base_dir'\n",
    "    # The 'sample_dir' parameter also needs to be removed from the call,\n",
    "    # as it's now internally derived from 'output_base_dir' and 'run_dir'.\n",
    "    resume_unet_checkpoint_path=None, # Set to None for training from scratch\n",
    "    verbose=False,\n",
    "    use_gradio=True  # <-- Enable Gradio here\n",
    ")\n",
    "\n",
    "# --- To resume training ---\n",
    "# Assuming you have a checkpoint saved from a previous run, e.g., unet_lora_weights_epoch_5.pth\n",
    "# demo = train_lora_diffusion(\n",
    "#     num_epochs=20, # Train for 5 more epochs\n",
    "#     train_dataloader=train_loader,\n",
    "#     valid_dataloader=valid_loader,\n",
    "#     dataset_for_sampling=train_dataset,\n",
    "#     learning_rate=2e-5, # You might want a lower LR when resuming\n",
    "#     lora_r=128,\n",
    "#     lora_alpha=128,\n",
    "#     lora_dropout=0.05,\n",
    "#     gradient_checkpoint_enable=True,\n",
    "#     gradient_accumulation_steps=4, # Example: accumulate gradients over 4 steps\n",
    "#     output_base_dir=Path(\"./lora_training_runs\"), # Change 'output_dir' to 'output_base_dir'\n",
    "#     # Remove 'sample_dir' here too\n",
    "#     resume_unet_checkpoint_path=Path(\"./lora_training_runs/YOUR_RUN_ID_HERE/lora_checkpoints/full_unet_checkpoint_epoch_X.pth\"), # Update path to reflect new structure\n",
    "#     verbose=False,\n",
    "#     use_gradio=True  # <-- Enable Gradio here\n",
    "# )\n",
    "\n",
    "if demo:\n",
    "    demo.launch(share=True) # Create public URL for Google Colab monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ce3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "##### SET RUN_ID TO RESUME FROM ######\n",
    "######################################\n",
    "RESUME_RUN_ID = 'af641a_run_20250805-173847'\n",
    "RESUME_EPOCH = 10\n",
    "\n",
    "# --- To train from scratch ---\n",
    "# demo = train_lora_diffusion(\n",
    "#     num_epochs=10,\n",
    "#     train_dataloader=train_loader,\n",
    "#     valid_dataloader=valid_loader,\n",
    "#     dataset_for_sampling=train_dataset,\n",
    "#     learning_rate=3e-5,\n",
    "#     lora_r=128,\n",
    "#     lora_alpha=128,\n",
    "#     lora_dropout=0.05,\n",
    "#     gradient_checkpoint_enable=True,\n",
    "#     gradient_accumulation_steps=4, # Example: accumulate gradients over 4 steps\n",
    "#     output_base_dir=Path(\"./lora_training_runs\"), # Change 'output_dir' to 'output_base_dir'\n",
    "#     # The 'sample_dir' parameter also needs to be removed from the call,\n",
    "#     # as it's now internally derived from 'output_base_dir' and 'run_dir'.\n",
    "#     resume_unet_checkpoint_path=None, # Set to None for training from scratch\n",
    "#     verbose=False,\n",
    "#     use_gradio=True  # <-- Enable Gradio here\n",
    "# )\n",
    "\n",
    "# --- To resume training ---\n",
    "# Assuming you have a checkpoint saved from a previous run, e.g., unet_lora_weights_epoch_5.pth\n",
    "demo = train_lora_diffusion(\n",
    "    num_epochs=30, # Train for 5 more epochs\n",
    "    train_dataloader=train_loader,\n",
    "    valid_dataloader=valid_loader,\n",
    "    dataset_for_sampling=train_dataset,\n",
    "    learning_rate=5e-5, # You might want a lower LR when resuming\n",
    "    lora_r=128,\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.05,\n",
    "    gradient_checkpoint_enable=True,\n",
    "    gradient_accumulation_steps=4, # Example: accumulate gradients over 4 steps\n",
    "    output_base_dir=Path(\"./lora_training_runs\"), # Change 'output_dir' to 'output_base_dir'\n",
    "    # Remove 'sample_dir' here too\n",
    "    resume_unet_checkpoint_path=Path(f\"./lora_training_runs/{RESUME_RUN_ID}/lora_checkpoints/full_unet_checkpoint_epoch_{RESUME_EPOCH}.pth\"), # Update path to reflect new structure\n",
    "    verbose=False,\n",
    "    use_gradio=True  # <-- Enable Gradio here\n",
    ")\n",
    "\n",
    "if demo:\n",
    "    demo.launch(share=True) # Create public URL for Google Colab monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff7918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as PILImage\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "######################################\n",
    "##### SET RUN_ID TO RESUME FROM ######\n",
    "######################################\n",
    "# RESUME_RUN_ID = 'af641a_run_20250805-173847'\n",
    "# RESUME_EPOCH = 10\n",
    "\n",
    "# Assuming load_model, unload_model, CLIPTokenizer, ImageWithPromptDataset,\n",
    "# train_loader, df_md_train, ds_train, val_transform are either globally defined\n",
    "# or imported from your training script. If not, uncomment and initialize them.\n",
    "\n",
    "# Example re-initialization for dataset components if not globally available\n",
    "# tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "# dataset_root: Path = Path('./datasets/appa-real-dataset_v2')\n",
    "# labels_md_train = dataset_root / 'labels_metadata_train.csv'\n",
    "# ds_train = dataset_root / 'train_data'\n",
    "# df_md_train = pd.read_csv(labels_md_train)\n",
    "# val_transform = transforms.Compose([\n",
    "#     transforms.Resize((512, 512)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "# ])\n",
    "# dataset_for_inference = ImageWithPromptDataset(df_md_train, ds_train, tokenizer, transform=val_transform)\n",
    "\n",
    "# Removed the initial 'if pipe: unload_model(pipe=pipe)' as it's not needed\n",
    "# for a fresh inference run, and might cause an error if 'pipe' isn't defined.\n",
    "\n",
    "def generate_image_from_prompt(pipe: StableDiffusionPipeline, prompt: str) -> PILImage.Image:\n",
    "    with torch.no_grad():\n",
    "        pipe.to(\"cuda\") # Ensure the pipe is on the correct device\n",
    "        result = pipe(prompt, num_inference_steps=75, guidance_scale=7.5)\n",
    "    return result.images[0]\n",
    "\n",
    "# ---- Load Base Model ----\n",
    "pipe: StableDiffusionPipeline = load_model()\n",
    "pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
    "\n",
    "# ---- Load LoRA Weights ----\n",
    "lora_adapter_name = \"age_gender_lora\"\n",
    "\n",
    "# **CRITICAL FIX**: Correct the path for loading LoRA weights.\n",
    "# Replace 'YOUR_ACTUAL_RUN_ID' with the specific run ID from your training output (e.g., '24dc7b_run_20250803-204721').\n",
    "# Make sure to include the 'lora_checkpoints' subdirectory.\n",
    "lora_checkpoint_path = Path(f\"./lora_training_runs/{RESUME_RUN_ID}/lora_checkpoints/lora_adapters_epoch_{RESUME_EPOCH}\")\n",
    "\n",
    "if not lora_checkpoint_path.exists():\n",
    "    print(f\"‚ùå Error: LoRA checkpoint path not found at {lora_checkpoint_path}. Please verify the path and run ID.\")\n",
    "else:\n",
    "    print(f\"Loading LoRA weights from: {lora_checkpoint_path}\")\n",
    "    pipe.load_lora_weights(str(lora_checkpoint_path), adapter_name=lora_adapter_name)\n",
    "    pipe.set_adapters([lora_adapter_name])\n",
    "    print(f\"‚úÖ LoRA adapter '{lora_adapter_name}' loaded and set.\")\n",
    "\n",
    "    # ---- Access Dataset & Sample Prompt ----\n",
    "    # Determine which dataset to use for prompt generation (global train_loader.dataset or re-initialized dataset_for_inference)\n",
    "    dataset_for_prompt = locals().get('dataset_for_inference', None)\n",
    "    if dataset_for_prompt is None:\n",
    "        if 'train_loader' in globals():\n",
    "            dataset_for_prompt = train_loader.dataset\n",
    "        else:\n",
    "            print(\"‚ùå Error: Dataset for prompt generation (train_loader or dataset_for_inference) not found. Please ensure it's loaded.\")\n",
    "            exit() # Exit or handle the error appropriately\n",
    "\n",
    "    sample_row: dict[str, Any] = random.choice(dataset_for_prompt.df.to_dict(orient=\"records\"))\n",
    "    sample_prompt: str = dataset_for_prompt.build_prompt(sample_row)\n",
    "\n",
    "    print(f\"üîπ Prompt: {sample_prompt}\")\n",
    "    print(f\"üîπ LoRA Adapter: {lora_adapter_name}\")\n",
    "\n",
    "    # ---- Generate Image ----\n",
    "    img: PILImage.Image = generate_image_from_prompt(pipe, sample_prompt)\n",
    "\n",
    "    # ---- Print Image Resolution ----\n",
    "    print(f\"üîπ Image Resolution: {img.width}x{img.height}\")\n",
    "\n",
    "    # ---- Display Image ----\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Prompt: {sample_prompt}\\nLoRA: {lora_adapter_name} | Resolution: {img.width}x{img.height}\", fontsize=10)\n",
    "    plt.show()\n",
    "\n",
    "    # Unload model after use to free GPU memory\n",
    "    unload_model(pipe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
