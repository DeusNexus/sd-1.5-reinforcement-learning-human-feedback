{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afaabfb9",
   "metadata": {},
   "source": [
    "# RLHF Diffusion Fine-Tuning for Age, Gender, Ethnicity, Emotion\n",
    "We will:\n",
    "1. Load dataset and metadata.\n",
    "2. Preprocess images and labels.\n",
    "3. Inject LoRA into U-Net.\n",
    "4. Add custom label-conditioning embedding.\n",
    "5. Train the model.\n",
    "6. Inference for conditioned image generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ef49ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "# !pip install pandas Pillow tqdm torch torchvision transformers diffusers scikit-learn IProgress ipywidgets accelerate peft matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43d19878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Configuration\n",
    "import os\n",
    "import gc\n",
    "import uuid\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, List, Optional, Any\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor2_0, AttnProcessor\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8315e43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4065, 5), Valid: (1482, 5), Test: (1978, 5)\n"
     ]
    }
   ],
   "source": [
    "# RUN_ID: str = str(uuid.uuid4()).replace('-', '')[:6]\n",
    "# print(f\"RUN_ID: {RUN_ID}\")\n",
    "\n",
    "# Define Batch Size\n",
    "batch_size: int = 2\n",
    "\n",
    "# Dataset Paths\n",
    "dataset_root: Path = Path('./datasets/appa-real-dataset_v2')\n",
    "labels_md_train = dataset_root / 'labels_metadata_train.csv'\n",
    "labels_md_valid = dataset_root / 'labels_metadata_valid.csv'\n",
    "labels_md_test  = dataset_root / 'labels_metadata_test.csv'\n",
    "\n",
    "ds_train = dataset_root / 'train_data'\n",
    "ds_valid = dataset_root / 'valid_data'\n",
    "ds_test  = dataset_root / 'test_data'\n",
    "\n",
    "# Load Metadata\n",
    "df_md_train = pd.read_csv(labels_md_train)\n",
    "df_md_valid = pd.read_csv(labels_md_valid)\n",
    "df_md_test  = pd.read_csv(labels_md_test)\n",
    "\n",
    "print(f\"Train: {df_md_train.shape}, Valid: {df_md_valid.shape}, Test: {df_md_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a700d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_run_logging(\n",
    "    output_base_dir: Path,\n",
    "    resume_unet_checkpoint_path: Optional[Path] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Sets up the logging directory for a training run, generates a RUN_ID,\n",
    "    and determines if the run is new or resumed.\n",
    "\n",
    "    Args:\n",
    "        output_base_dir (Path): The base directory where all run data will be saved.\n",
    "        resume_unet_checkpoint_path (Optional[Path]): Path to a .pth file\n",
    "                                                      if resuming training.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing run_id, run_dir, history_file,\n",
    "                        and a boolean indicating if training is resumed.\n",
    "    \"\"\"\n",
    "    is_resumed_run = False\n",
    "    run_id = None\n",
    "    run_dir = None\n",
    "    history_file = None\n",
    "\n",
    "    if resume_unet_checkpoint_path and resume_unet_checkpoint_path.exists():\n",
    "        is_resumed_run = True\n",
    "        # Extract RUN_ID from the parent directory of the checkpoint path\n",
    "        # Assuming checkpoints are saved in a run-specific directory\n",
    "        parts = resume_unet_checkpoint_path.parts\n",
    "        # Find the part that looks like a RUN_ID (e.g., '1a2b3c_run_YYYYMMDD-HHMMSS')\n",
    "        for part in reversed(parts):\n",
    "            if '_run_' in part and len(part.split('_')[0]) == 6: # Heuristic for 6-char UUID prefix\n",
    "                run_id = part\n",
    "                break\n",
    "        if run_id is None:\n",
    "            # Fallback if the expected RUN_ID format isn't found in path\n",
    "            print(\"Warning: Could not extract RUN_ID from resume path. Generating new one.\")\n",
    "            run_id = f\"{str(uuid.uuid4()).replace('-', '')[:6]}_run_{pd.Timestamp.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "        \n",
    "        # Reconstruct run_dir based on the identified run_id within the output_base_dir\n",
    "        run_dir = output_base_dir / run_id\n",
    "        if not run_dir.exists():\n",
    "            print(f\"Warning: Resuming from checkpoint but expected run directory {run_dir} not found. Creating it.\")\n",
    "            run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        history_file = run_dir / \"training_history.json\"\n",
    "        print(f\"üîÑ Resuming existing run with RUN_ID: {run_id}\")\n",
    "    else:\n",
    "        # New run\n",
    "        run_id = f\"{str(uuid.uuid4()).replace('-', '')[:6]}_run_{pd.Timestamp.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "        run_dir = output_base_dir / run_id\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "        history_file = run_dir / \"training_history.json\"\n",
    "        # Initialize history file for new runs\n",
    "        with open(history_file, 'w') as f:\n",
    "            json.dump({\"epochs\": []}, f)\n",
    "        print(f\"‚ú® Starting new run with RUN_ID: {run_id}\")\n",
    "\n",
    "    # Create subdirectories for checkpoints and samples within the run_dir\n",
    "    (run_dir / \"lora_checkpoints\").mkdir(exist_ok=True)\n",
    "    (run_dir / \"lora_samples\").mkdir(exist_ok=True)\n",
    "\n",
    "    return {\n",
    "        \"run_id\": run_id,\n",
    "        \"run_dir\": run_dir,\n",
    "        \"history_file\": history_file,\n",
    "        \"is_resumed_run\": is_resumed_run\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83cc27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageWithPromptDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_md: pd.DataFrame,\n",
    "        images_dir: Path,\n",
    "        tokenizer: CLIPTokenizer,\n",
    "        transform: Optional[transforms.Compose],\n",
    "    ):\n",
    "        self.df = df_md\n",
    "        self.images_dir = images_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def build_prompt(self,row):\n",
    "        age_desc = f\"{int(row['age'])} years old\"\n",
    "        gender_desc = row['gender']\n",
    "        ethnicity_desc = row['ethnicity']\n",
    "\n",
    "        # Emotion mapping\n",
    "        emotion_map = {\n",
    "            'neutral': \"with a neutral expression\",\n",
    "            'happy': \"smiling happily\",\n",
    "            'slightlyhappy': \"smiling slightly\",\n",
    "            'other': \"showing a subtle emotion\"\n",
    "        }\n",
    "        emotion_desc = emotion_map.get(row['emotion'], \"with an expression\")  # fallback if unknown\n",
    "\n",
    "        prompt = f\"A {age_desc} {ethnicity_desc} {gender_desc} {emotion_desc}\"\n",
    "        return prompt\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = f\"{int(row['imageId']):06d}.jpg\"\n",
    "        img_path = self.images_dir / img_name\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        prompt = self.build_prompt(row)\n",
    "        prompt_ids = self.tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=77).input_ids[0]\n",
    "\n",
    "        return {\n",
    "            'pixel_values': image,\n",
    "            'prompt_ids': prompt_ids\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0115ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transforms & Loaders\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),\n",
    "    transforms.Resize((512, 512)), # Stable Diffusion 1.5 resolution\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),\n",
    "    transforms.Resize((512, 512)), # Stable Diffusion 1.5 resolution\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "train_dataset = ImageWithPromptDataset(df_md_train, ds_train, tokenizer, transform=train_transform)\n",
    "valid_dataset = ImageWithPromptDataset(df_md_valid, ds_valid, tokenizer, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae340d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(lora_r: int = 64, lora_alpha: int = 64, lora_dropout: float = 0.05, device: str = 'cuda', verbose:bool = False) -> StableDiffusionPipeline:\n",
    "    \"\"\"\n",
    "    Loads the Stable Diffusion pipeline with LoRA configuration and ensures correct trainable parameters.\n",
    "\n",
    "    Args:\n",
    "        lora_r (int): LoRA rank.\n",
    "        lora_alpha (int): LoRA alpha scaling.\n",
    "        lora_dropout (float): Dropout probability for LoRA.\n",
    "        device (str): Device to load the model onto.\n",
    "\n",
    "    Returns:\n",
    "        StableDiffusionPipeline: The configured pipeline ready for training.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Load base pipeline\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(device)\n",
    "    print(\"‚úÖ Base model loaded.\")\n",
    "\n",
    "    # 2. Define LoRA PEFT config\n",
    "    peft_lora_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=[\"to_q\", \"to_v\"],\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\"\n",
    "    )\n",
    "\n",
    "    # 3. Add LoRA Adapter\n",
    "    pipe.unet.add_adapter(adapter_name=\"age_gender_lora\", adapter_config=peft_lora_config)\n",
    "    print(\"‚úÖ LoRA adapter added.\")\n",
    "\n",
    "    # 4. Enable LoRA layers for training\n",
    "    pipe.unet.enable_lora()\n",
    "    print(\"‚úÖ LoRA adapter enabled for training.\")\n",
    "\n",
    "    # 5. Freeze VAE & Text Encoder\n",
    "    pipe.vae.requires_grad_(False)\n",
    "    pipe.text_encoder.requires_grad_(False)\n",
    "    print(\"‚úÖ VAE and Text Encoder frozen.\")\n",
    "\n",
    "    # 6. Debug LoRA Injection\n",
    "    print(\"\\nüîç Checking LoRA Layers Injected into UNet...\")\n",
    "    lora_param_count = 0\n",
    "    for name, param in pipe.unet.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            param_dtype = param.dtype\n",
    "            nan_inf = torch.isnan(param).any() or torch.isinf(param).any()\n",
    "            if verbose: print(f\" - {name}: shape={param.shape}, mean={param.data.mean():.6f}, std={param.data.std():.6f}, dtype={param_dtype}, requires_grad={param.requires_grad}, nan/inf={nan_inf}\")\n",
    "            lora_param_count += 1\n",
    "\n",
    "    if lora_param_count == 0:\n",
    "        raise RuntimeError(\"‚ùå No LoRA parameters found in UNet! Injection failed.\")\n",
    "\n",
    "    # 7. Ensure ONLY LoRA Layers Are Trainable\n",
    "    non_lora_trainable = [n for n, p in pipe.unet.named_parameters() if p.requires_grad and 'lora' not in n]\n",
    "    if non_lora_trainable:\n",
    "        print(\"‚ùå ERROR: Found non-LoRA parameters set to requires_grad=True:\")\n",
    "        for n in non_lora_trainable:\n",
    "            print(f\" - {n}\")\n",
    "        raise RuntimeError(\"Non-LoRA params are trainable! You must freeze them explicitly.\")\n",
    "    else:\n",
    "        print(\"‚úÖ Only LoRA layers are trainable.\")\n",
    "\n",
    "    print(\"\\nüöÄ LoRA setup successful. Ready for training loop.\")\n",
    "    return pipe\n",
    "\n",
    "def unload_model(pipe: StableDiffusionPipeline) -> None:\n",
    "    \"\"\"\n",
    "    Unloads the UNet from the pipeline and clears GPU memory.\n",
    "\n",
    "    Args:\n",
    "        pipe (StableDiffusionPipeline): The current pipeline to unload from.\n",
    "    \"\"\"\n",
    "    print(\"üîª Unloading UNet to free GPU memory...\")\n",
    "    del pipe.unet\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"‚úÖ UNet unloaded and GPU cache cleared.\")\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "## Updated `train_lora_diffusion` function with Gradient Checkpointing\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "def train_lora_diffusion(\n",
    "    num_epochs: int,\n",
    "    train_dataloader: DataLoader,\n",
    "    valid_dataloader: DataLoader,\n",
    "    dataset_for_sampling: Dataset,\n",
    "    learning_rate: float = 3e-5,\n",
    "    lora_r: int = 64,\n",
    "    lora_alpha: int = 64,\n",
    "    lora_dropout: float = 0.05,\n",
    "    gradient_checkpoint_enable: bool = True, # New parameter for gradient checkpointing\n",
    "    gradient_accumulation_steps: int = 1, # Added gradient accumulation steps for completeness\n",
    "    output_base_dir: Path = Path(\"./lora_training_runs\"), # Changed to base dir\n",
    "    resume_unet_checkpoint_path: Optional[Path] = None,\n",
    "    device: str = 'cuda',\n",
    "    verbose: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Trains a Stable Diffusion model with LoRA for age, gender, ethnicity, and emotion conditioning.\n",
    "    Can save both LoRA adapter weights and full U-Net checkpoints for resuming.\n",
    "\n",
    "    Args:\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        train_dataloader (DataLoader): DataLoader for the training dataset.\n",
    "        valid_dataloader (DataLoader): DataLoader for the validation dataset.\n",
    "        dataset_for_sampling (Dataset): The dataset object used for generating random prompts for sample images.\n",
    "        learning_rate (float): Learning rate for the AdamW optimizer.\n",
    "        lora_r (int): LoRA rank.\n",
    "        lora_alpha (int): LoRA alpha scaling.\n",
    "        lora_dropout (float): Dropout probability for LoRA.\n",
    "        gradient_checkpoint_enable (bool): Whether to enable gradient checkpointing to save GPU memory.\n",
    "        output_dir (Path): Directory to save all checkpoints.\n",
    "        sample_dir (Path): Directory to save generated sample images.\n",
    "        resume_unet_checkpoint_path (Optional[Path]): Path to a .pth file containing the full U-Net state (including LoRA) to resume training from. If None, train from scratch.\n",
    "        device (str): Device to perform training on ('cuda' or 'cpu').\n",
    "        verbose (bool): If True, prints additional details during initialization.\n",
    "        gradient_accumulation_steps (int): Number of updates steps to accumulate gradients before performing a backward/update pass.\n",
    "                                           Used to simulate a larger effective batch size.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Setup Run Logging ---\n",
    "    run_info = setup_run_logging(output_base_dir, resume_unet_checkpoint_path)\n",
    "    run_id = run_info[\"run_id\"]\n",
    "    run_dir = run_info[\"run_dir\"]\n",
    "    history_file = run_info[\"history_file\"]\n",
    "    is_resumed_run = run_info[\"is_resumed_run\"]\n",
    "\n",
    "    # Adjust output_dir and sample_dir based on the specific run directory\n",
    "    output_dir = run_dir / \"lora_checkpoints\"\n",
    "    sample_dir = run_dir / \"lora_samples\"\n",
    "\n",
    "    # Load existing history or start new\n",
    "    training_history = {\"epochs\": []}\n",
    "    if is_resumed_run and history_file.exists():\n",
    "        with open(history_file, 'r') as f:\n",
    "            training_history = json.load(f)\n",
    "        print(f\"üìä Loaded existing training history from {history_file}\")\n",
    "    \n",
    "    # Determine starting epoch for resumed runs\n",
    "    start_epoch = 0\n",
    "    if is_resumed_run and training_history[\"epochs\"]:\n",
    "        # Assuming each entry in \"epochs\" corresponds to one epoch\n",
    "        start_epoch = len(training_history[\"epochs\"])\n",
    "        print(f\"Resuming from epoch {start_epoch + 1}.\")\n",
    "\n",
    "    pipe: StableDiffusionPipeline = load_model(lora_r=lora_r, lora_alpha=lora_alpha, lora_dropout=lora_dropout, device=device)\n",
    "    pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
    "\n",
    "    # --- Enable gradient checkpointing (conditional) ---\n",
    "    if gradient_checkpoint_enable:\n",
    "        pipe.unet.enable_gradient_checkpointing()\n",
    "        print(f\"‚úÖ Gradient checkpointing enabled for UNet: gradient_accumulation_steps = {gradient_accumulation_steps}\")\n",
    "\n",
    "    # Load full U-Net checkpoint if resuming training\n",
    "    if resume_unet_checkpoint_path is not None: # Changed from != None to is not None for better practice\n",
    "        if resume_unet_checkpoint_path.exists():\n",
    "            print(f\"üîÑ Resuming training from full U-Net checkpoint: {resume_unet_checkpoint_path}\")\n",
    "            # Load the entire UNet state_dict\n",
    "            unet_state_dict = torch.load(resume_unet_checkpoint_path, map_location='cpu')\n",
    "            pipe.unet.load_state_dict(unet_state_dict)\n",
    "            print(\"‚úÖ Full U-Net state loaded for resuming training.\")\n",
    "        else:\n",
    "            print(f\"‚ùå Resume U-Net checkpoint not found at {resume_unet_checkpoint_path}. Starting training from scratch.\")\n",
    "\n",
    "    scaler = torch.amp.GradScaler(device)\n",
    "\n",
    "    # Initialize LoRA_B weights if zero (for new LoRA layers or when starting from scratch)\n",
    "    for name, module in pipe.unet.named_modules():\n",
    "        if hasattr(module, 'lora_B'):\n",
    "            if isinstance(module.lora_B, torch.nn.ModuleDict):\n",
    "                for sub_name, sub_module in module.lora_B.items():\n",
    "                    if hasattr(sub_module, 'weight'):\n",
    "                        if sub_module.weight.abs().sum() == 0:\n",
    "                            torch.nn.init.kaiming_uniform_(sub_module.weight)\n",
    "                            if verbose: print(f\"Initialized lora_B: {name}.{sub_name}\")\n",
    "            elif hasattr(module.lora_B, 'weight'):\n",
    "                if module.lora_B.weight.abs().sum() == 0:\n",
    "                    torch.nn.init.kaiming_uniform_(module.lora_B.weight)\n",
    "                    if verbose: print(f\"Initialized lora_B: {name}\")\n",
    "\n",
    "    lora_params = [p for n, p in pipe.unet.named_parameters() if p.requires_grad]\n",
    "    for p in lora_params:\n",
    "        p.data = p.data.float()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(lora_params, lr=learning_rate)\n",
    "\n",
    "    print(f\"\\nüöÄ Starting training for {num_epochs} epochs...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        pipe.unet.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_idx, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "            pixel_values = batch['pixel_values'].to(device, dtype=torch.float32)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                latents = pipe.vae.to(dtype=torch.float32).encode(pixel_values).latent_dist.sample()\n",
    "                latents = latents.to(dtype=torch.float16) * 0.18215\n",
    "\n",
    "            if torch.isnan(latents).any() or torch.isinf(latents).any():\n",
    "                print(f\"[Batch {batch_idx}] Latents NaN/Inf Detected! Skipping batch.\")\n",
    "                # This could cause issues with gradient accumulation if you skip a batch without accumulating\n",
    "                # Consider adding a check for this if accumulation is enabled\n",
    "                continue\n",
    "\n",
    "            noise = torch.randn_like(latents)\n",
    "            if torch.isnan(noise).any() or torch.isinf(noise).any():\n",
    "                print(f\"[Batch {batch_idx}] Noise NaN/Inf Detected! Skipping batch.\")\n",
    "                continue\n",
    "\n",
    "            timesteps = torch.randint(10, pipe.scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n",
    "            noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            if torch.isnan(noisy_latents).any() or torch.isinf(noisy_latents).any():\n",
    "                print(f\"[Batch {batch_idx}] Noisy Latents NaN/Inf Detected! Skipping batch.\")\n",
    "                continue\n",
    "\n",
    "            prompt_ids = batch['prompt_ids'].to(device)\n",
    "            with torch.no_grad():\n",
    "                encoder_hidden_states = pipe.text_encoder(prompt_ids)[0]\n",
    "\n",
    "            # No optimizer.zero_grad() here; it's moved inside the accumulation block\n",
    "\n",
    "            with torch.amp.autocast(device, dtype=torch.float16):\n",
    "                model_pred = pipe.unet(noisy_latents, timesteps, encoder_hidden_states=encoder_hidden_states).sample\n",
    "\n",
    "                if torch.isnan(model_pred).any() or torch.isinf(model_pred).any():\n",
    "                    print(f\"[Batch {batch_idx}] Model Prediction NaN/Inf Detected! Mean: {model_pred.mean()}, Std: {model_pred.std()}. Skipping batch.\")\n",
    "                    continue\n",
    "\n",
    "                loss = nn.functional.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "                # Scale the loss down by gradient_accumulation_steps\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"[Batch {batch_idx}] Loss NaN/Inf Detected! Skipping batch.\")\n",
    "                    continue\n",
    "\n",
    "            scaler.scale(loss).backward() # This accumulates gradients\n",
    "\n",
    "            # Accumulate loss for logging\n",
    "            total_loss += loss.item() * gradient_accumulation_steps # Un-scale loss for total calculation\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"  [Batch {batch_idx}] Loss: {loss.item() * gradient_accumulation_steps:.6f}\") # Display un-scaled loss\n",
    "\n",
    "\n",
    "            # --- Gradient Accumulation Logic ---\n",
    "            # Only perform optimizer step and scaler update every `gradient_accumulation_steps` batches\n",
    "            # or on the last batch of the epoch\n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == len(train_dataloader):\n",
    "                # Check for NaN/Inf gradients before stepping (important for stability)\n",
    "                skip_step = False\n",
    "                for name, param in pipe.unet.named_parameters():\n",
    "                    if param.requires_grad and param.grad is not None:\n",
    "                        if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                            print(f\"[Batch {batch_idx}] NaN/Inf gradient detected in {name}. Skipping optimizer step.\")\n",
    "                            skip_step = True\n",
    "                            break\n",
    "\n",
    "                if skip_step:\n",
    "                    # If skipping, clear gradients to prevent accumulation with bad gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    continue\n",
    "\n",
    "                scaler.unscale_(optimizer) # Unscale gradients before clipping\n",
    "                torch.nn.utils.clip_grad_norm_(lora_params, max_norm=1.0) # Clip gradients to prevent explosion\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update() # Update the scale for the next iteration\n",
    "                optimizer.zero_grad() # Reset gradients after performing a step\n",
    "\n",
    "        # End of epoch, perform a final step if any accumulated gradients are left\n",
    "        # (This is handled by the `(batch_idx + 1) == len(train_dataloader)` condition in the loop)\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "        # # --- Save ONLY LoRA adapter weights (diffusers format) ---\n",
    "        # lora_adapter_save_path = output_dir / f\"lora_adapters_epoch_{epoch+1}\"\n",
    "        # lora_adapter_save_path.mkdir(parents=True, exist_ok=True)\n",
    "        # pipe.unet.save_pretrained(lora_adapter_save_path, adapter_name=\"age_gender_lora\")\n",
    "        # print(f\"Saved LoRA adapter weights only at {lora_adapter_save_path}\")\n",
    "\n",
    "        # # --- Save FULL U-Net state_dict (including LoRA state) ---\n",
    "        # full_unet_checkpoint_path = output_dir / f\"full_unet_checkpoint_epoch_{epoch+1}.pth\"\n",
    "        # torch.save(pipe.unet.state_dict(), full_unet_checkpoint_path)\n",
    "        # print(f\"Saved full U-Net checkpoint (including LoRA state) at {full_unet_checkpoint_path}\")\n",
    "\n",
    "        # --- Save Checkpoints for this epoch ---\n",
    "        epoch_lora_adapter_save_path = output_dir / f\"lora_adapters_epoch_{epoch+1}\"\n",
    "        epoch_lora_adapter_save_path.mkdir(parents=True, exist_ok=True)\n",
    "        pipe.unet.save_pretrained(epoch_lora_adapter_save_path, adapter_name=\"age_gender_lora\")\n",
    "        print(f\"Saved LoRA adapter weights only at {epoch_lora_adapter_save_path}\")\n",
    "\n",
    "        full_unet_checkpoint_path = output_dir / f\"full_unet_checkpoint_epoch_{epoch+1}.pth\"\n",
    "        torch.save(pipe.unet.state_dict(), full_unet_checkpoint_path)\n",
    "        print(f\"Saved full U-Net checkpoint (including LoRA state) at {full_unet_checkpoint_path}\")\n",
    "\n",
    "        # ---- Generate Sample Image ----\n",
    "        sample_row = random.choice(dataset_for_sampling.df.to_dict(orient=\"records\"))\n",
    "        sample_prompt:str = dataset_for_sampling.build_prompt(sample_row)\n",
    "        generator = torch.Generator(device=device).manual_seed(42)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.amp.autocast(device):\n",
    "                image = pipe(prompt=sample_prompt, num_inference_steps=75, guidance_scale=7.5, generator=generator).images[0]\n",
    "\n",
    "        # Save the image\n",
    "        image_save_path = sample_dir / f\"sample_epoch_{epoch+1}.png\"\n",
    "        image.save(image_save_path)\n",
    "        print(f\"Saved sample image at {image_save_path}\")\n",
    "\n",
    "        # Save the prompt to a text file in the sample_dir\n",
    "        prompt_save_path = sample_dir / f\"sample_epoch_{epoch+1}_prompt.txt\"\n",
    "        with open(prompt_save_path, 'w') as file:\n",
    "            file.write(sample_prompt)\n",
    "        print(f\"Saved prompt to {prompt_save_path}\")\n",
    "\n",
    "        # --- Display Image and Prompt using Matplotlib ---\n",
    "        plt.figure(figsize=(8, 8)) # Adjust figure size as needed\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Epoch {epoch+1} Sample\\nPrompt: {sample_prompt}\", wrap=True) # wrap=True for long prompts\n",
    "        plt.axis('off') # Hide axes\n",
    "        plt.show() # Display the plot\n",
    "\n",
    "        # For Jupyter/Colab notebooks, `display` can sometimes help with rendering order\n",
    "        # display(plt.gcf()) # Get current figure and display it\n",
    "        # plt.close(plt.gcf()) # Close the plot to free memory and avoid displaying it twice\n",
    "\n",
    "        print(f\"Prompt: {sample_prompt}\") # This line is redundant if saved, but fine for immediate output\n",
    "\n",
    "        # --- Save History for the current epoch ---\n",
    "        epoch_history = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": avg_loss,\n",
    "            \"sample_prompt\": sample_prompt,\n",
    "            \"sample_image_path\": str(image_save_path),\n",
    "            \"lora_adapter_path\": str(epoch_lora_adapter_save_path),\n",
    "            \"full_unet_checkpoint_path\": str(full_unet_checkpoint_path)\n",
    "        }\n",
    "        training_history[\"epochs\"].append(epoch_history)\n",
    "\n",
    "        with open(history_file, 'w') as f:\n",
    "            json.dump(training_history, f, indent=4)\n",
    "        print(f\"Updated training history saved to {history_file}\")\n",
    "\n",
    "    unload_model(pipe)\n",
    "    print(\"‚ú® Training complete! Model unloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c67befda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 37 years old caucasian female smiling slightly\n"
     ]
    }
   ],
   "source": [
    "# Random Sample of ImageDataSet Prompt\n",
    "dataset = train_loader.dataset\n",
    "sample_row = random.choice(dataset.df.to_dict(orient=\"records\"))\n",
    "sample_prompt = dataset.build_prompt(sample_row)\n",
    "print(sample_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce2e97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ú® Starting new run with RUN_ID: 760a85_run_20250803-202647\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecde593c56fa46ebbe50b4754b3ba85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Base model loaded.\n",
      "‚úÖ LoRA adapter added.\n",
      "‚úÖ LoRA adapter enabled for training.\n",
      "‚úÖ VAE and Text Encoder frozen.\n",
      "\n",
      "üîç Checking LoRA Layers Injected into UNet...\n",
      "‚úÖ Only LoRA layers are trainable.\n",
      "\n",
      "üöÄ LoRA setup successful. Ready for training loop.\n",
      "‚úÖ Gradient checkpointing enabled for UNet: gradient_accumulation_steps = 4\n",
      "\n",
      "üöÄ Starting training for 5 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 1/2033 [00:01<44:12,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Batch 0] Loss: 0.090369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   3%|‚ñé         | 69/2033 [01:05<30:59,  1.06it/s]"
     ]
    }
   ],
   "source": [
    "# --- To train from scratch ---\n",
    "train_lora_diffusion(\n",
    "    num_epochs=5,\n",
    "    train_dataloader=train_loader,\n",
    "    valid_dataloader=valid_loader,\n",
    "    dataset_for_sampling=train_dataset,\n",
    "    learning_rate=3e-5,\n",
    "    lora_r=128,\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.05,\n",
    "    gradient_checkpoint_enable=True,\n",
    "    gradient_accumulation_steps=4, # Example: accumulate gradients over 4 steps\n",
    "    output_base_dir=Path(\"./lora_training_runs\"), # Change 'output_dir' to 'output_base_dir'\n",
    "    # The 'sample_dir' parameter also needs to be removed from the call,\n",
    "    # as it's now internally derived from 'output_base_dir' and 'run_dir'.\n",
    "    resume_unet_checkpoint_path=None, # Set to None for training from scratch\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# --- To resume training ---\n",
    "# Assuming you have a checkpoint saved from a previous run, e.g., unet_lora_weights_epoch_5.pth\n",
    "# train_lora_diffusion(\n",
    "#     num_epochs=20, # Train for 5 more epochs\n",
    "#     train_dataloader=train_loader,\n",
    "#     valid_dataloader=valid_loader,\n",
    "#     dataset_for_sampling=train_dataset,\n",
    "#     learning_rate=1e-5, # You might want a lower LR when resuming\n",
    "#     lora_r=128,\n",
    "#     lora_alpha=128,\n",
    "#     lora_dropout=0.05,\n",
    "#     gradient_checkpoint_enable=True,\n",
    "#     gradient_accumulation_steps=4, # Example: accumulate gradients over 4 steps\n",
    "#     output_base_dir=Path(\"./lora_training_runs\"), # Change 'output_dir' to 'output_base_dir'\n",
    "#     # Remove 'sample_dir' here too\n",
    "#     resume_unet_checkpoint_path=Path(\"./lora_training_runs/YOUR_RUN_ID_HERE/lora_checkpoints/full_unet_checkpoint_epoch_X.pth\"), # Update path to reflect new structure\n",
    "#     verbose=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff7918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "if pipe:\n",
    "    unload_model(pipe=pipe)\n",
    "\n",
    "def generate_image_from_prompt(pipe: StableDiffusionPipeline, prompt: str) -> Image.Image:\n",
    "    with torch.no_grad():\n",
    "        result = pipe(prompt, num_inference_steps=75)\n",
    "    return result.images[0]\n",
    "\n",
    "# ---- Load Base Model ----\n",
    "pipe: StableDiffusionPipeline = load_model()\n",
    "pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
    "\n",
    "# ---- Load LoRA Weights ----\n",
    "lora_adapter_name = \"age_gender_lora\"\n",
    "pipe.load_lora_weights(\"./lora_checkpoint/unet_lora_weights_epoch_10.pth\", adapter_name=lora_adapter_name)\n",
    "pipe.set_adapters([lora_adapter_name])\n",
    "\n",
    "# ---- Access Dataset & Sample Prompt ----\n",
    "dataset: Dataset = train_loader.dataset\n",
    "sample_row: dict[str, Any] = random.choice(dataset.df.to_dict(orient=\"records\"))\n",
    "sample_prompt: str = dataset.build_prompt(sample_row)\n",
    "\n",
    "print(f\"üîπ Prompt: {sample_prompt}\")\n",
    "print(f\"üîπ LoRA Adapter: {lora_adapter_name}\")\n",
    "\n",
    "# ---- Generate Image ----\n",
    "img: Image.Image = generate_image_from_prompt(pipe, sample_prompt)\n",
    "\n",
    "# ---- Print Image Resolution ----\n",
    "print(f\"üîπ Image Resolution: {img.width}x{img.height}\")\n",
    "\n",
    "# ---- Display Image ----\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Hide axis ticks\n",
    "plt.title(f\"Prompt: {sample_prompt}\\nLoRA: {lora_adapter_name} | Resolution: {img.width}x{img.height}\", fontsize=10)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
